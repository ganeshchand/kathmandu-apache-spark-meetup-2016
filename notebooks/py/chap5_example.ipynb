{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Learning Spark Chapter 5, Loading and Saving your Data, Examples in Python**\n\n[![Learning Spark](http://akamaicovers.oreilly.com/images/0636920028512/cat.gif)](http://www.jdoqocy.com/click-7645222-11260198?url=http%3A%2F%2Fshop.oreilly.com%2Fproduct%2F0636920028512.do%3Fcmp%3Daf-strata-books-videos-product_cj_9781449358600_%2525zp&cjsku=0636920028512)\n\nMany of the examples in Chapter 5 require access to certain storage systems, these examples have been left out. For Databricks Cloud you should consider using the table creation mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 setup\n\nNote: If you have '/' characters in your secret access key, they must be escaped with '%2F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"YOUR ACCESS KEY GOES HERE\"\n",
    "SECRET_KEY = \"YOUR SECRET KEY GOES HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5-1. Python load text file example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all text files from the directory and do a word count on them. With the (k,v) pair RDD, make Rows of RDDs and create a dataframe. \nSave it as a temp table and issue some example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import Row\n",
    "from pyspark.sql import *\n",
    "\n",
    "input = sc.textFile(\"file:///dbfs/learning-spark-master/files/*.txt\")\n",
    "inputWords = input.flatMap(lambda l: l.split()).map(lambda w: w.lower())\n",
    "pairs = inputWords.map(lambda w: (w, 1))\n",
    "wordCount = pairs.reduceByKey(lambda x, y: x + y)\n",
    "wordRowRDD = wordCount.map(lambda p: Row(word= p[0], value=p[1]))\n",
    "wordDF = sqlContext.createDataFrame(wordRowRDD)\n",
    "display(wordDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register dataframe as a temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDF.registerTempTable(\"wordcount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue some SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select word, value from wordcount where value >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "display (wordDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5-6. Python load unstructured JSON example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkFiles\n",
    "from pyspark import SparkFiles\n",
    "import json\n",
    "# Fetch the remote example since it isn't on local FS or S3\n",
    "# Load the file into an RDD\n",
    "jsonInput = sc.textFile(\"file:///dbfs/learning-spark-master/files/testweet.json\")\n",
    "jsonInput.collect()\n",
    "#Parse it\n",
    "data = jsonInput.map(lambda x: json.loads(x))\n",
    "# Collect the parsed results back to the driver\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "name": "chap5_example",
  "notebookId": 87878.0
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
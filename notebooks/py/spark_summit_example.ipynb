{"cells":[{"cell_type":"markdown","source":["Some examples from the Spark Summit class where the differecne between reduceByKey and groupByKey operations illustrate the subtle difference.\nFor small datasets this may not be a sigficant performance and network traffic hit, but for large sets, you do want to use reduceByKey operation.\n\nAlso, good illustration of the difference between map() and flatMap() transformations and how to use them against different types of RDDs"],"metadata":{}},{"cell_type":"markdown","source":["Parallelize some numbers across a few partitions, in this case let the default be 2, I believe"],"metadata":{}},{"cell_type":"code","source":["baseRDD = sc.parallelize(range(10))\nbaseRDD.collect()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Use lambda function first to create a tuple of three elements, and then use flatmap on it to create a list of tranformed values."],"metadata":{}},{"cell_type":"code","source":["transRDD = baseRDD.flatMap(lambda n: (n, n*100, 42))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["we have two rdds, one the base, and the second transformed in value, with its tuples flattend out by using flatMap() operations"],"metadata":{}},{"cell_type":"code","source":["print (baseRDD.collect())\nprint (transRDD.collect())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["create a triples RDD with tuples of 3 elements."],"metadata":{}},{"cell_type":"code","source":["triplesRDD = baseRDD.map(lambda n: (n, n*100, 42))\ntriplesRDD.collect()\n\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Using flatMap we create yet another RDD which is now flattaned."],"metadata":{}},{"cell_type":"code","source":["flatTriplesRDD = triplesRDD.flatMap(lambda n: n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["print (baseRDD.collect())\nprint (transRDD.collect())\nprint (flatTriplesRDD.collect())"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["An illustration of how to filter out unwanted elements"],"metadata":{}},{"cell_type":"code","source":["nonZeroRDD = flatTriplesRDD.filter(lambda n: n > 0)\nnonZeroRDD.collect()\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["sorted = nonZeroRDD.collect()\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["sorted\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["The list is now sorted using Python method on the list.sor()"],"metadata":{}},{"cell_type":"code","source":["sorted.sort()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["As we can see, since this is a Python notebook and sorted is at list, we can employ its sort() method, which transforms the list in place"],"metadata":{}},{"cell_type":"code","source":["print sorted"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["GroupBy allows us the take an existing RDD and create another one arranged or groupedBy a key of our liking. In this case, we can use the first letter of the name.\nCreate an RDD as a list of names."],"metadata":{}},{"cell_type":"code","source":["namesRDD = sc.parallelize([\"Jules\", \"James\", \"John\", \"Adam\", \"Mary\", \"Jane\", \"Anna\", \"Fred\", \"Bruce\", \"Hasan\", \"Felicia\", \"Mona\", \"Mary\", \"Achim\", \"Angelica\", \"Mike\", \"Kirk\", \"Karl\", \"Ryan\", \"Jim\", \"Kay\", \"Marie\", \"Vista\", \"Victor\", \"Xaviar\", \"Crista\"])"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["transfrom them by grouping them into (Key, Value), where Key is the first letter of the name, while value is the list of names that begin with the letter Key."],"metadata":{}},{"cell_type":"code","source":["namesByGroupRDD = namesRDD.groupBy(lambda n: n[0])"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["type(namesByGroupRDD)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Python way to print them out."],"metadata":{}},{"cell_type":"code","source":["print [(k , list(v)) for (k, v) in namesByGroupRDD.collect()]"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["another example where we have an existing RDD that is already a (K,V) format."],"metadata":{}},{"cell_type":"code","source":["pairsRDD = sc.parallelize([('A', 4), ('C', 1), ('B', 1), ('F', 2), ('H', 1), ('K', 2), ('J', 5), ('A', 3), ('B', 2)])\n                      "],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["As before, we group them by key, so all values for a key are group together as a list. For example, (A, [4, 3])"],"metadata":{}},{"cell_type":"code","source":["pairsByKeyRDD = pairsRDD.groupByKey()\nprint (list((j[0], list(j[1])) for j in pairsByKeyRDD.collect()))\n"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["A simple way to demonstrate the succintness of WordCount in Spark. Amazingly is small amount of code!"],"metadata":{}},{"cell_type":"markdown","source":["create an word RDD."],"metadata":{}},{"cell_type":"code","source":["words = sc.parallelize([\"one\", \"two\", \"two\", \"three\", \"three\", \"three\"])"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Transform the RDD into (key, value) pair"],"metadata":{}},{"cell_type":"code","source":["wordsPairs = words.map(lambda w: (w, 1))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Use reduceByKey() transformation to add the count"],"metadata":{}},{"cell_type":"code","source":["wordsCount = wordsPairs.reduceByKey(lambda a, b: a + b)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["wordsCount.collect()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Another less efficient way to to use groupByKey() transformation. Note that the former is a lot effecient and results in less network traffic during shuffle stage"],"metadata":{}},{"cell_type":"code","source":["wordsCountByGroup = wordsPairs.groupByKey().map(lambda t: (t[0], sum(t[1])))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["As you can see the results are identical"],"metadata":{}},{"cell_type":"code","source":["wordsCountByGroup.collect()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":44}],"metadata":{"name":"spark_summit_example","notebookId":4376},"nbformat":4,"nbformat_minor":0}

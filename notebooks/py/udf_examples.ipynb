{"cells":[{"cell_type":"markdown","source":["# **Download Large SQL Queries in Python**\n* When using the Databricks Cloud UI to download the results of a SQL query, the results are limited to the first 1000 rows.\n* This example contains code to help you access the entire dataset.\n* First, the notebook walks through an explanation how to do this.\n* At the end, we give a simple function you can run in your notebooks to do this.\n* I have modified and added some code?udfs defined and used?to make it more interesting -- Jules"],"metadata":{}},{"cell_type":"markdown","source":["### **Setup**\nLet's create a SQL table with more than 1000 rows."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\n##square the numbers\ndef squared (s):\n  return s * s\n#\n## cube the number\n#\ndef cubed (s):\n  return s * s * s\n#\n## register them as UDF so we can use them in SQL\n#Row(key=\"b\", group=\"consonants\", value=2),\nsqlContext.udf.register(\"squaredWithPython\", squared)\nsqlContext.udf.register(\"cubedWithPython\", cubed)\n##now create some rows \narray = []\nfor i in range(0, 5000):\n  array.append(Row(num=i, square=squared(i), cube=cubed(i)))\n#create a Dataframe\ndataFrame = sqlContext.createDataFrame(sc.parallelize(array))\n#register the Dataframe as temporary table to which we can issue some SQL queries.\n#\ndataFrame.registerTempTable(\"test_download_table\")\n## use UDF now for another table\nsqlContext.range(1, 20).registerTempTable(\"test_udfs\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sql \n-- Clicking on the download button on the bottom right for this table will download only the first 1000 rows.\nselect * from test_download_table"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Use UDFS defined above with test_udf tables"],"metadata":{}},{"cell_type":"code","source":["%sql describe test_udfs"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sql select * from test_udfs"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql select id, squaredWithPython(id) as id_squared from test_udfs\n-- use the UDF defined above to square numbers in the test_udfs table"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql select id, cubedWithPython(id) as id_cubed from test_udfs\n--use the udf defined above to cube the numbers in the test_udfs table"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Solution 1: Make the output results available for all Databricks users.\n\nIf you only need to allow Databricks users to read these output files, then you can save the file to our [FileStore](\"group_id\") and download it there."],"metadata":{}},{"cell_type":"code","source":["sql_query = \"select * from test_download_table\"\ndbutils.fs.rm(\"dbfs:/FileStore/test_download_table\", True)\nsqlContext.sql(sql_query).coalesce(1).write.format(\"com.databricks.spark.csv\").save(\"dbfs:/FileStore/test_download_table\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["You can access a file in the File store by going to [files/test_download_table/part-00000](files/test_download_table/part-00000)"],"metadata":{}},{"cell_type":"markdown","source":["### Solution 2: Make the output results universally accessible.\n\nIf you want the files to be accessible by non-Databricks users, following these steps:"],"metadata":{}},{"cell_type":"markdown","source":["#### Step 1: Configure your results to be saved in an S3 bucket."],"metadata":{}},{"cell_type":"code","source":["ACCESS_KEY = \"YOUR_ACCESS_KEY\"\nSECRET_KEY = \"YOUR_SECRET_KEY\"\nAWS_BUCKET_NAME = \"YOUR_AWS_BUCKET_NAME\"\nFOLDER_FOR_RESULTS = \"YOUR_DESTINATION_FOLDER\""],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#### Step 2: Save the results to an S3 bucket.\n\nThe **saveAsTextFile** command is safe even on a very large RDD as each worker writes out it's partitions directly."],"metadata":{}},{"cell_type":"code","source":["sql_query = \"select * from test_download_table\"\nsqlContext.sql(sql_query).coalesce(1).write.format(\"com.databricks.spark.csv\").save(\"s3n://%s:%s@%s/%s\" % (ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME, FOLDER_FOR_RESULTS))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#### Step 3: Call the S3 library to generate links for downloading these files.\n\nSet the desired timeout you would like the links to be valid for."],"metadata":{}},{"cell_type":"code","source":["from boto.s3.connection import S3Connection\nconn = S3Connection(ACCESS_KEY, SECRET_KEY)\nbucket = conn.get_bucket(AWS_BUCKET_NAME)\n# NOTE: There are as many output files as there were partitions in the RDD.\n# coalesce() or repartition() can be called on RDD to explicity set the desired number of output files.\nrs = bucket.list(\"%s/part\" % FOLDER_FOR_RESULTS)\n\nhtml_output = \"\"\nfor key in rs:\n  html_output += \"<a href='%s'>Download %s</a><br/>\" % (key.generate_url(expires_in=1440, query_auth=True), key.name)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#### Step 4: Use displayHTML to show the clickable download links."],"metadata":{}},{"cell_type":"code","source":["displayHTML(html_output)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["#### **Final: Put this together into a function that can be called to create the displayed links.**\nUse the **output_links_to_download_sql** function from now on rather than all the steps above."],"metadata":{}},{"cell_type":"code","source":["def output_links_to_download_sql(sql_query, num_output_files, accesskey, secretkey, aws_bucket_name, aws_folder):\n  sqlContext.sql(sql_query).coalesce(num_output_files).write.format(\"com.databricks.spark.csv\").save(\"s3n://%s:%s@%s/%s\" % (accesskey, secretkey, aws_bucket_name, aws_folder))\n  from boto.s3.connection import S3Connection\n  conn = S3Connection(accesskey, secretkey)\n  bucket = conn.get_bucket(aws_bucket_name)\n  rs = bucket.list(\"%s/part\" % aws_folder)\n  html_output = \"\"\n  for key in rs:\n    html_output += \"<a href='%s'>Download %s</a><br/>\" % (key.generate_url(expires_in=1440, query_auth=True), key.name)\n  displayHTML(html_output)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["output_links_to_download_sql(\"select * from test_download_table\", 1, ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME, \"test-sql-download\")"],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"7 Download All SQL - py","notebookId":3838},"nbformat":4,"nbformat_minor":0}

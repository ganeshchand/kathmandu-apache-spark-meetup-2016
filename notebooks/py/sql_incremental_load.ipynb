{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Incremental Load\n\nThis notebook describes how to take a set of records that represents the updates to a SQL Table and computes what the table looks like at the current point in time.\nMoreover, it defines python function that are used in flatmap instead of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference pyspark.sql.Row object\n",
    "from pyspark.sql import Row\n",
    "# Build the array of Row objects that represents the UserDataHistory table.\n",
    "# one can think of rows as really how you want your table to look like as rows & columns, and you\n",
    "# can them the schema that matches your sql table model in your RDMS.\n",
    "array = [Row(action=\"Insert\", modified_at=\"2-15-01-01 1 pm\", user_id=\"1\", name=\"Bob\", value=150),\n",
    "         Row(action=\"Update\", modified_at=\"2-15-02-01 2 pm\", user_id=\"1\", name=\"Robert\", value=200),\n",
    "         Row(action=\"Delete\", modified_at=\"2-15-02-02 4 pm\", user_id=\"1\", name=\"Robert\", value=200),\n",
    "         Row(action=\"Insert\", modified_at=\"2-15-01-01 1 pm\", user_id=\"2\", name=\"Liz\", value=150),\n",
    "         Row(action=\"Update\", modified_at=\"2-15-01-01 2 pm\", user_id=\"2\", name=\"Elizabeth\", value=160)]\n",
    "\n",
    "# Create RDD using sc.parallelize and then transforms it into a DataFrame\n",
    "# note we have stringed together three operaitons in one line. \n",
    "# 1. parallelize and RDD\n",
    "# 2. Convert or create the RDD as the Dataframe with the schema as defined by each Row object\n",
    "# 3. Register the Dataframe as a temporary table, to which can issue SQL queries\n",
    "df = sqlContext.createDataFrame(sc.parallelize(array))\n",
    "df.registerTempTable(\"UserDataHistory\")\n",
    "##use some API calls on dataframes\n",
    "#get the count\n",
    "df.count()\n",
    "# get the columns\n",
    "df.columns\n",
    "# return all the rows\n",
    "df.collect()\n",
    "# show\n",
    "df.show()\n",
    "# explain\n",
    "df.explain(True)\n",
    "#print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql describe UserDataHistory\n",
    "-- describe the table. As we can see it matches our schema specified in the Row objects above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from UserDataHistory order by user_id, modified_at asc\n",
    "-- select all rows from the UserDataHistory, order them by user_id and modified time in an ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from UserDataHistory order by value asc\n",
    "-- this time do the same thing but order them by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a Row object return a list [id, row]\n",
    "def convertRowToPair(row):\n",
    "  return [row.user_id, row]\n",
    "convertRowToPair(Row(action=\"Insert\", modified_at=\"2-15-01-01 1 pm\", user_id=\"1\", name=\"Bob\", value=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFinalRow(pair):\n",
    "  id = pair[0]\n",
    "  rows = pair[1]\n",
    "  final_row = None\n",
    "  for r in rows:\n",
    "    if final_row == None:\n",
    "      final_row = r\n",
    "      continue\n",
    "    if r.modified_at > final_row:\n",
    "      final_row = r\n",
    "  if final_row == None:\n",
    "    return []\n",
    "  elif final_row.action == \"Delete\":\n",
    "    return []\n",
    "  else:\n",
    "    return [final_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transforming int k,v pairs really\n",
    "sqlContext.sql(\"select * from UserDataHistory\").rdd.map(convertRowToPair).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from UserDataHistory\").rdd.map(convertRowToPair).groupByKey().flatMap(lambda p: p).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = sqlContext.sql(\"select * from UserDataHistory\").rdd.map(convertRowToPair).groupByKey().flatMap(getFinalRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.createDataFrame(final_table).registerTempTable(\"UserData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from UserData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "name": "9 SQL Incremental Load",
  "notebookId": 3941.0
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
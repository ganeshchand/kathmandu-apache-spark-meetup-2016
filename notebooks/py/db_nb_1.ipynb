{"cells":[{"cell_type":"markdown","source":["![Databricks Logo](http://training.databricks.com/databricks_guide/databricks_logo_400px.png)\n# **Introduction to Python Notebooks** \n\n* This introduction notebook describes how to get started running Python code in Notebooks.\n* If you have not already done so, please review the [Welcome to Databricks](/#workspace/databricks_guide/00 Welcome to Databricks) guide."],"metadata":{}},{"cell_type":"markdown","source":["### **Clone** this notebook\nThis is a locked notebook so you will need to clone it into your workspace to use it.  \n* Click the **Clone** button in the menu bar on the top left. \n* Navigate to the cloned notebook (e.g. click Menu > Workspace > ``Intro Python Notebooks copy``)\n\n![Menu Bar Clone Notebook](http://training.databricks.com/databricks_guide/unlock_nb_clone_menu.png)"],"metadata":{}},{"cell_type":"markdown","source":["### **Attach** the Notebook to a **cluster**\n* A **Cluster** is a group of machines which can run commands in cells.\n* Check the upper left corner of your notebook to see if it is **Attached** or **Detached**.\n* If **Detached**, select a cluster to attach it to. If you don't currently have clusters, create one as described in the [Welcome to Databricks](/#workspace/databricks_guide/00 Welcome to Databricks) guide.\n\n![Attach Notebook](http://training.databricks.com/databricks_guide/cluster_attach2.png)"],"metadata":{}},{"cell_type":"markdown","source":["***\n#### ![Quick Note](http://training.databricks.com/databricks_guide/icon_note3_s.png) **Cells** are units that make up notebooks\n![A Cell](http://training.databricks.com/databricks_guide/cell.png)\n\nCells each have a type - either **scala**, **python**, **sql**, **R**, or **markdown**.\n* While cells default to the type of the Notebook, other cell types are supported as well.\n* For example, Python Notebooks can contain python, sql, or markdown cells but not scala cells.\n* This cell is in **markdown** and is used for documentation. [Markdown](http://en.wikipedia.org/wiki/Markdown) is a simple text formatting syntax.\n***"],"metadata":{}},{"cell_type":"markdown","source":["### ** Create** and **Run** a New Markdown Cell in this Notebook\n* When you mouse between cells, a + sign will pop up in the center that you can click on to create a new cell.\n\n ![New Cell](http://training.databricks.com/databricks_guide/create_new_cell.png)\n* Type **``%md Hello, world!``** into your new cell (**``%md``** indicates the cell is markdown).\n\n\n\n* Press **Shift+Enter** when in the cell to **run** it and proceed to the next cell.\n  * The cells contents should update.\n  \n  ![Run cell](http://training.databricks.com/databricks_guide/run_cell.png)"],"metadata":{}},{"cell_type":"markdown","source":["Jules, Welcome to the DB Notebook cloud"],"metadata":{}},{"cell_type":"markdown","source":["Hello, world!"],"metadata":{}},{"cell_type":"markdown","source":["***\n#### ![Quick Note](http://training.databricks.com/databricks_guide/icon_note3_s.png) **Markdown Cell Tips**\n* To change a non-markdown cell to markdown, add **%md** to very start of the cell.\n* After updating the contents of a markdown cell, the cell must be run again to update the formatted contents of a markdown cell.\n* Cells are not automatically run each time you open it.\n  * Instead, previous results from running a cell are saved and displayed.\n* Alternately, press **Ctrl+Enter** when in a cell to **run** it, but not proceed to the next cell.\n***"],"metadata":{}},{"cell_type":"markdown","source":["### Run a **Python Cell**\n* Run the following python cell.\n* Note: There is no need for any special indicator (such as %md) necessary to create a Python cell in a Python notebook.\n* Make sure the output date and time updates before moving on."],"metadata":{}},{"cell_type":"code","source":["# This is a Python cell.\nimport datetime\nprint \"This was last run on: %s\" % datetime.datetime.now()\nprint \"Hello Jules, Welcome to the Databricks Cloud\""],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Running **Spark**\nThe variable **sc** allows you to access a Spark Context to run your Spark programs.\n* For more information about Spark, please refer to [Spark Overview](https://spark.apache.org/docs/latest/)"],"metadata":{}},{"cell_type":"code","source":["words = sc.parallelize([\"hello\", \"world\", \"goodbye\", \"hello\", \"again\", \"cats\", \"dogs\", \"rats\", \"snakes\", \"python\", \"java\", \"python\", \"go\"])\nwordcounts = words.map(lambda s: (s, 1)).reduceByKey(lambda a, b : a + b).collect()\nprint wordcounts"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Exercise: Calculate the number of unique words in the \"words\" RDD here.\n# (Hint: The answer should be 4.)\nunique = words.distinct().collect()\nprint unique\nprint len(unique)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Exercise: Create an RDD of numbers, and use Spark to find the mean.\nfrom operator import add\nnumbersRDD = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n#not the most efficient way since we're invoking two actions: reduce, and count\n#must be an efficient way to do it.\nsum = numbersRDD.sum()\ncount = numbersRDD.count()\nprint sum, count, float (sum / count)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["&nbsp;\n\n### Working with **DataFrames**\n* Python can be used to create Spark [DataFrames](http://spark.apache.org/docs/latest/sql-programming-guide.html) - a distributed collection of data organized into named columns.\n* DataFrames are created by calling ``createDataFrame`` on an RDD of pyspark.sql.Row objects."],"metadata":{}},{"cell_type":"code","source":["# Reference pyspark.sql.Row object\nfrom pyspark.sql import Row\n\n# Build the array of Row objects\narray = [Row(key=\"a\", group=\"vowels\", value=105),\n         Row(key=\"b\", group=\"consonants\", value=25),\n         Row(key=\"c\", group=\"consonants\", value=30),\n         Row(key=\"d\", group=\"consonants\", value=45),\n         Row(key=\"e\", group=\"vowels\", value=55)]\n# build the array of Row of people objects\npeople = [Row(key=1, age=32, name=\"Jules Damji\", city=\"San Francisco\", occupation=\"advocate\"),\n          Row(key=2,  age=42, name=\"Arther Dent\", city=\"London\", occupation=\"Hitch-hiker\"),\n          Row(key=3,  age=2009, name=\"Zaphod Beeblebrox\", city=\"Milkyway\", occupation=\"Wiseman\"),\n          Row(key=4,  age=55, name=\"Ford Prefect\", city=\"Essex\", occupation=\"Realtor\"),\n          Row(key=5, age=2005, name=\"Trillian\", city=\"Andromeda\", occupation=\"Space Traveler\"),\n          Row(key=6, age=1000, name=\"Slartibarfast\", city=\"M33\", occupation=\"Creator\"),\n          Row(key=7, age=4500, name=\"Random Dent\", city=\"X33\", occupation=\"Invisble Hand\")\n]\n\n# Create RDD using sc.parallelize and then transforms it into a DataFrame\ndf = sqlContext.createDataFrame(sc.parallelize(array))\ndfp = sqlContext.createDataFrame(sc.parallelize(people))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["#### **Show this data**\nUse the **``display``** command to view a DataFrame in a notebook."],"metadata":{}},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["***\n#### ![Quick Note](http://training.databricks.com/databricks_guide/icon_note3_s.png) **The visualization above is interactive**\n* Click on the **Chart Button** ![Chart Button](http://training.databricks.com/databricks_guide/chart_button.png) to toggle the view.\n* Try different types of graphs by clicking on the arrow next to the chart button.\n* If you have selected a graph, you can click on **Plot Options...** for even more ways to customize the view.\n***"],"metadata":{}},{"cell_type":"code","source":["# Exercise: Create a DataFrame and display it. \n# Can you use the \"Plot Options\" to plot the group vs. the sum of the values in that group?\n# (Hint: Vowels = 6, and consonants = 9)\ndisplay(df)\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(dfp)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["***\n## ![Quick Note](http://training.databricks.com/databricks_guide/icon_note3_s.png) **Where to Go Next**\n\nWe've now covered the basics of a Databricks Python Notebook.  Practice the optional exercises below for more python exercises, look to the following notebooks in other languages, or go to the Databricks Product Overview.\n* [Notebook Tutorials in Scala](/#workspace/databricks_guide/01 Intro Notebooks/2 Intro Scala Notebooks)\n* [Notebook Tutorials in SQL](/#workspace/databricks_guide/01 Intro Notebooks/3 Intro SQL Notebooks)\n* [Notebook Tutorials in R](/#workspace/databricks_guide/01 Intro Notebooks/4 Intro R Notebooks)\n* [Databricks Product Overview](/#workspace/databricks_guide/02 Product Overview/00 Product Overview)\n***"],"metadata":{}},{"cell_type":"markdown","source":["## **Optional Tasks**"],"metadata":{}},{"cell_type":"markdown","source":["### **Importing Standard Python libraries**\n* For other libraries that are not available by default, you can upload other libraries to the Workspace.\n* Refer to the **[Libraries](/#workspace/databricks_guide/02 Product Overview/07 Libraries)** guide for more details."],"metadata":{}},{"cell_type":"code","source":["import re\nm = re.search('(?<=abc)def', 'abcdef')\nm.group(0)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### **Using Spark SQL within a Python Notebook**\nYou can use execute SQL commands within a python notebook by invoking **``%sql``**."],"metadata":{}},{"cell_type":"code","source":["%sql show databases"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["#### **Use ``registerTempTable`` on a DataFrame** \n* Register the above DataFrame table (built in python) for SQL queries.\n* Temporary tables are not meant to be persistent, i.e. they will not survive cluster restarts."],"metadata":{}},{"cell_type":"code","source":["df.registerTempTable(\"PythonTempTable\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%sql describe PythonTempTable\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["dfp.registerTempTable(\"PeopleTable\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["#### **Visualizations and Spark SQL**\n* A visualization appears automatically for the output of a **SQL select** statement in notebooks (no need to call ``display``)."],"metadata":{}},{"cell_type":"code","source":["%sql select * from PythonTempTable"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["&nbsp;\n\n#### ** Persist DataFrames into Tables **\nUse **``saveAsTable``** to persist tables to be used in other notebooks.\n* These table definitions will persist even after cluster restarts."],"metadata":{}},{"cell_type":"code","source":["\ndf.write.saveAsTable(\"PythonTestTable\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["***\n![Quick Note](http://training.databricks.com/databricks_guide/icon_note3_s.png) For more information on working with Python and Spark SQL, please refer to [Spark SQL Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n***"],"metadata":{}},{"cell_type":"markdown","source":["### ** Display HTML **\nDisplay HTML within your notebook, using the **displayHTML** command."],"metadata":{}},{"cell_type":"code","source":["displayHTML(\"<h3 style=\\\"color:blue\\\">Blue Text</h3>\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":39}],"metadata":{"name":"1 Intro Python Notebooks (2)","notebookId":3596},"nbformat":4,"nbformat_minor":0}

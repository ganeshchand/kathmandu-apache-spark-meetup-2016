{"cells":[{"cell_type":"markdown","source":["# ** Spark DataFrames with Python **\nThis notebook demonstrates a lot of common Spark DataFrames functions with Python. Nice example to see how you can use transformation on Dataframes, create or save as temporary tables, and then issue SQL queires against them"],"metadata":{}},{"cell_type":"code","source":["# import pyspark class row from module sql\nfrom pyspark.sql import *\n\n# Create Example Data - Departments and Employees\n\n# Create the Departments or you could read these from a csv or json file\ndepartment1 = Row(id='123456', name='Computer Science')\ndepartment2 = Row(id='789012', name='Mechanical Engineering')\ndepartment3 = Row(id='345678', name='Theater and Drama')\ndepartment4 = Row(id='901234', name='Indoor Recreation')\n\n# Create the Employees\nemployee1 = Row(firstName='michael', lastName='armbrust', email='no-reply@berkeley.edu', salary=100000)\nemployee2 = Row(firstName='chris', lastName='fregly', email='no-reply@northwestern.edu', salary=120000)\nemployee3 = Row(firstName='matei', lastName=None, email='no-reply@waterloo.edu', salary=140000)\nemployee4 = Row(firstName=None, lastName='wendell', email='no-reply@berkeley.edu', salary=160000)\n\n# Create the DepartmentWithEmployees instances from Departments and Employees\ndepartmentWithEmployees1 = Row (department = department1, employees = [employee1, employee2])\ndepartmentWithEmployees2 = Row (department = department2, employees = [employee3, employee4])\ndepartmentWithEmployees3 = Row (department = department3, employees = [employee1, employee4])\ndepartmentWithEmployees4 = Row (department = department4, employees = [employee2, employee3])\n\nprint department1\nprint departmentWithEmployees1.employees[0].email\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["print departmentWithEmployees1\nprint departmentWithEmployees2"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["** Create the first DataFrame from a List of the Case Classes. **"],"metadata":{}},{"cell_type":"code","source":["departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\ndf1 = sqlContext.createDataFrame(departmentsWithEmployeesSeq1)\n\ndisplay (df1)\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["** Create a 2nd DataFrame from a List of Case Classes. **"],"metadata":{}},{"cell_type":"code","source":["departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\ndf2 = sqlContext.createDataFrame(departmentsWithEmployeesSeq2)\n\ndisplay(df2)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## ** Working with DataFrames **"],"metadata":{}},{"cell_type":"markdown","source":["** Union 2 DataFrames. **"],"metadata":{}},{"cell_type":"code","source":["unionDF = df1.unionAll(df2)\ndisplay(unionDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["** Write the Unioned DataFrame to a Parquet file. **"],"metadata":{}},{"cell_type":"code","source":["# Remove the file if it exists\ndbutils.fs.rm(\"/tmp/databricks-df-example.parquet\", True)\nunionDF.write.parquet(\"/tmp/databricks-df-example.parquet\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["** Read a DataFrame from the Parquet file that you wrote to. **"],"metadata":{}},{"cell_type":"code","source":["parquetDF = sqlContext.read.parquet(\"/tmp/databricks-df-example.parquet\")\ndisplay (parquetDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["**Explode the employees column.** \nExplode Function: https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html#explode"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\nfrom pyspark.sql import Row\n#the lambda function gets each row, and what you end up with is python list of employee rows\nparquetDF.select(explode(\"employees\").alias(\"employee\")).map(lambda row: row).collect()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\n##create another dataframe with columns firstname, lastname, email & salary\n## this time the lambda function gets a row, but explodes it into its respective fields. Note that \"None\" maps to [] or empty string\nexplodeDF = sqlContext.createDataFrame(parquetDF.select(explode(\"employees\").alias(\"employee\")).map(lambda row: Row(row.employee.firstName, row.employee.lastName, row.employee.email, row.employee.salary)), [\"firstName\", \"lastName\", \"email\", \"salary\"])\n\ndisplay(explodeDF)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["print explodeDF"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["**Use ``filter()`` to return only the rows that match the given predicate. Note that we can now employ transformations on our Dataframe as with RDDs. What's returned is antoher Dataframe**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, asc\n##use filter transformation on Dataframe and produce another dataframe\nfilterDF = explodeDF.filter(col(\"firstName\") == \"chris\").sort(col(\"lastName\"))\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["from pyspark.sql.functions import col, asc\n##a more complicated filter with predicates and generate another dataframe\nfilterDF = explodeDF.filter((col(\"firstName\") == \"chris\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["**The ``where()`` clause is equivalent to ``filter()``.**"],"metadata":{}},{"cell_type":"code","source":["whereDF = explodeDF.where((col(\"firstName\") == \"chris\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(whereDF)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["**Replace ``null`` values with empty string using DataFrame Na functions.**"],"metadata":{}},{"cell_type":"code","source":["naFunctions = explodeDF.na\nnonNullDF = naFunctions.fill(\"\")\n\ndisplay(nonNullDF)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["**Retrieve only rows with missing firstName or lastName.**"],"metadata":{}},{"cell_type":"code","source":["##powerfully declarative and descriptive way to winnow down your dataframe\nfilterNonNullDF = nonNullDF.filter((col(\"firstName\") == \"\") | (col(\"lastName\") == \"\")).sort(asc(\"email\"))\ndisplay(filterNonNullDF)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["**Example aggregations using ``agg()`` and ``countDistinct()``.**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\ncountDistinctDF = nonNullDF.select(\"firstName\", \"lastName\").groupBy(\"firstName\", \"lastName\").agg(countDistinct(\"firstName\")) \n  \ndisplay(countDistinctDF)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["**Compare the DataFrame and SQL Query Physical Plans**\n**(Hint:  They should be the same.)**"],"metadata":{}},{"cell_type":"code","source":["countDistinctDF.explain()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# register the DataFrame as a temp table so that we can query it using SQL\nnonNullDF.registerTempTable(\"databricks_df_example\")\n\n# Perform the same query as the DataFrame above and return ``explain``\ncountDistinctDF_sql = sqlContext.sql(\"SELECT firstName, lastName, count(distinct firstName) as distinct_first_names FROM databricks_df_example GROUP BY firstName, lastName\")\n\ncountDistinctDF_sql.explain()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["**Sum up all the salaries**"],"metadata":{}},{"cell_type":"code","source":["salarySumDF = nonNullDF.agg({\"salary\" : \"sum\"}) \ndisplay(salarySumDF)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["type(nonNullDF.salary)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["**Print the summary statistics for the salaries.**"],"metadata":{}},{"cell_type":"code","source":["nonNullDF.describe(\"salary\").show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### An example using Pandas & Matplotlib Integration"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport matplotlib.pyplot as plt\nplt.clf()\npdDF = nonNullDF.toPandas()\ntype(pdDF)\npdDF.plot(x='firstName', y='salary', kind='line')\nplt.clf()\npdDF.plot(x='firstName', y='salary', kind='bar')\ndisplay()\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["### Cleanup: Remove the parquet file."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(\"/tmp/databricks-df-example.parquet\", True)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":42}],"metadata":{"name":"2 DataFrames - py (2)","notebookId":4142},"nbformat":4,"nbformat_minor":0}
